{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atikhasan007/Scikit-learn/blob/main/kaggol%20copy%20code%20and%20help%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def create_mask_from_npz(npz_file_path):\n",
        "\n",
        "    \"\"\"Creates a binary mask from polygons stored in an NPZ file.\n",
        "\n",
        "    Args:\n",
        "        npz_file_path (str): Path to the NPZ file containing polygon data.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The binary mask (256x256).\n",
        "    \"\"\"\n",
        "\n",
        "    # Load data from the NPZ file\n",
        "    with np.load(npz_file_path) as f:\n",
        "        # Get all arrays from the NPZ file\n",
        "        polygon_arrays = [v for k, v in f.items() if k.startswith('arr_')]\n",
        "\n",
        "    # Create an empty mask (black) with the fixed dimensions\n",
        "    mask = np.zeros((256, 256), dtype=np.uint8)  # Use uint8 for binary mask\n",
        "\n",
        "    # Iterate over each polygon array and fill the mask\n",
        "    for polygons in polygon_arrays:\n",
        "        if polygons.ndim == 2 and polygons.shape[1] == 2:  # Check if valid polygon format\n",
        "            mask = cv2.fillPoly(mask, pts=[polygons], color=255) # White for filled area\n",
        "\n",
        "    # Apply Gaussian blur for anti-aliasing (optional)\n",
        "    #mask = cv2.GaussianBlur(mask, (0, 0), sigmaX=2, sigmaY=2, borderType=cv2.BORDER_DEFAULT)\n",
        "    mask = mask / mask.max()\n",
        "    return mask\n",
        "\n",
        "# usage\n",
        "npz_file = \"/kaggle/input/preprocessed-change/change_detection_dataset_poly_preprocessed/label/79114_50991.npz\"\n",
        "result_mask = create_mask_from_npz(npz_file)\n",
        "\n",
        "# can save or display the results\n",
        "plt.imshow(result_mask,cmap=\"gray\")\n"
      ],
      "metadata": {
        "id": "kQGGON6VWRe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, image_dir_A, image_dir_B, mask_dir, transform=None):\n",
        "        self.image_dir_A = image_dir_A\n",
        "        self.image_dir_B = image_dir_B\n",
        "        sclass ChangeDetectionDataset(Dataset):\n",
        " elf.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.image_names = os.listdir(image_dir_A)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_A_name = self.image_names[idx]\n",
        "        img_B_name = img_A_name  # Assuming both image sets have the same filenames\n",
        "\n",
        "        img_A_path = os.path.join(self.image_dir_A, img_A_name)\n",
        "        img_B_path = os.path.join(self.image_dir_B, img_B_name)\n",
        "        mask_path = os.path.join(self.mask_dir, os.path.splitext(img_A_name)[0] + '.npz')\n",
        "\n",
        "        image_A = Image.open(img_A_path).convert('RGB')\n",
        "        image_B = Image.open(img_B_path).convert('RGB')\n",
        "\n",
        "        mask = create_mask_from_npz(mask_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image_A = self.transform(image_A)\n",
        "            image_B = self.transform(image_B)\n",
        "            mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
        "\n",
        "        return image_A, image_B, mask\n"
      ],
      "metadata": {
        "id": "sAuLQ4q-WRcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paths\n",
        "image_dir_A = '/kaggle/input/preprocessed-change/change_detection_dataset_poly_preprocessed/newA'\n",
        "image_dir_B = '/kaggle/input/preprocessed-change/change_detection_dataset_poly_preprocessed/newB'\n",
        "mask_dir = '/kaggle/input/preprocessed-change/change_detection_dataset_poly_preprocessed/label'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Creating dataset\n",
        "dataset = ChangeDetectionDataset(image_dir_A, image_dir_B, mask_dir, transform=transform)"
      ],
      "metadata": {
        "id": "hNubV2WNWRZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_samples(dataset, num_samples=5):\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n",
        "    dataset_size = len(dataset)\n",
        "    indices = np.random.choice(dataset_size, num_samples, replace=False)\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        image_A, image_B, mask = dataset[idx]\n",
        "        image_A = image_A.permute(1, 2, 0).numpy()\n",
        "        image_B = image_B.permute(1, 2, 0).numpy()\n",
        "        mask = mask.squeeze(0).numpy()\n",
        "\n",
        "        axes[i, 0].imshow(image_A)\n",
        "        axes[i, 0].set_title('Image A')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        axes[i, 1].imshow(image_B)\n",
        "        axes[i, 1].set_title('Image B')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        axes[i, 2].imshow(mask, cmap='gray')\n",
        "        axes[i, 2].set_title('Mask')\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_samples(dataset)\n"
      ],
      "metadata": {
        "id": "1GAOFj0GWRWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        def upconv_block(in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        self.encoder1 = conv_block(in_channels, 64)\n",
        "        self.encoder2 = conv_block(64, 128)\n",
        "        self.encoder3 = conv_block(128, 256)\n",
        "        self.encoder4 = conv_block(256, 512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        self.bottleneck = conv_block(512, 1024)\n",
        "\n",
        "        self.upconv4 = upconv_block(1024, 512)\n",
        "        self.decoder4 = conv_block(1024, 512)\n",
        "        self.upconv3 = upconv_block(512, 256)\n",
        "        self.decoder3 = conv_block(512, 256)\n",
        "        self.upconv2 = upconv_block(256, 128)\n",
        "        self.decoder2 = conv_block(256, 128)\n",
        "        self.upconv1 = upconv_block(128, 64)\n",
        "        self.decoder1 = conv_block(128, 64)\n",
        "\n",
        "        self.conv_last = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool(enc1))\n",
        "        enc3 = self.encoder3(self.pool(enc2))\n",
        "        enc4 = self.encoder4(self.pool(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = self.decoder4(torch.cat((dec4, enc4), dim=1))\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = self.decoder3(torch.cat((dec3, enc3), dim=1))\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = self.decoder2(torch.cat((dec2, enc2), dim=1))\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = self.decoder1(torch.cat((dec1, enc1), dim=1))\n",
        "\n",
        "        return self.conv_last(dec1)"
      ],
      "metadata": {
        "id": "XbmEjrQkWRUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Creating data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Initializing the model, loss function, and optimizer\n",
        "model = UNet(in_channels=6, out_channels=1).cuda()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    for images_A, images_B, masks in tqdm(train_loader):\n",
        "        images_A = images_A.cuda()\n",
        "        images_B = images_B.cuda()\n",
        "        masks = masks.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images_A, images_B)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images_A, images_B, masks in val_loader:\n",
        "            images_A = images_A.cuda()\n",
        "            images_B = images_B.cuda()\n",
        "            masks = masks.cuda()\n",
        "\n",
        "            outputs = model(images_A, images_B)\n",
        "            loss = criterion(outputs, masks)\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = running_val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}\")\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "inference_times = []\n",
        "with torch.no_grad():\n",
        "    for images_A, images_B, masks in tqdm(test_loader):\n",
        "        images_A = images_A.cuda()\n",
        "        images_B = images_B.cuda()\n",
        "        masks = masks.cuda()\n",
        "\n",
        "        start_time = time.time()\n",
        "        outputs = model(images_A, images_B)\n",
        "        end_time = time.time()\n",
        "\n",
        "        inference_time = end_time - start_time\n",
        "        inference_times.append(inference_time)\n",
        "\n",
        "        loss = criterion(outputs, masks)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "avg_inference_time = np.mean(inference_times)\n",
        "print(f\"Test Loss: {avg_test_loss}\")\n",
        "print(f\"Average Inference Time per Batch: {avg_inference_time:.6f} seconds\")\n"
      ],
      "metadata": {
        "id": "uqEwXaqtWRRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_change_mask(model, image_A_path, image_B_path, transform, device=\"cuda\"):\n",
        "    # Loading and preprocessing images\n",
        "    image_A = Image.open(image_A_path).convert('RGB')\n",
        "    image_B = Image.open(image_B_path).convert('RGB')\n",
        "    image_A = transform(image_A).unsqueeze(0).to(device)\n",
        "    image_B = transform(image_B).unsqueeze(0).to(device)\n",
        "\n",
        "    # predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(image_A, image_B)\n",
        "\n",
        "    # Threshold and convertion to binary mask\n",
        "    mask = (output > 0.05).float().cpu().numpy()[0, 0]\n",
        "    return mask"
      ],
      "metadata": {
        "id": "XB3fliyEWROw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction Example\n",
        "image_A_path = \"/kaggle/input/preprocessed-change/change_detection_dataset_poly_preprocessed/newA/79120_50988.jpg\"\n",
        "image_B_path = \"/kaggle/input/preprocessed-change/change_detection_dataset_poly_preprocessed/newB/79120_50988.jpg\"\n",
        "\n",
        "change_mask = predict_change_mask(model, image_A_path, image_B_path, transform)\n",
        "\n",
        "plt.imshow(change_mask, cmap='gray')"
      ],
      "metadata": {
        "id": "mZG3UMxvWRL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3p_XJjTWRJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uoYUA2hPWRGo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}