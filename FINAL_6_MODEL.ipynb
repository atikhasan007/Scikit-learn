{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhfcKjuq9MSiz50D38kP3/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atikhasan007/Scikit-learn/blob/main/FINAL_6_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxJrLxvi3jVZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        os.path.join(dirname, filename)\n",
        "print(\"Files added\")"
      ],
      "metadata": {
        "id": "naVfLgjd3uKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, UpSampling2D, concatenate, Input, Lambda, Dropout, MaxPooling2D, Conv2DTranspose\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "h3meUjKR3uH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unet_model(input_size=(256, 256, 6)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Split the input into two separate RGB images\n",
        "    input_image1 = inputs[..., :3]\n",
        "    input_image2 = inputs[..., 3:]\n",
        "\n",
        "    # Encoder for image 1\n",
        "    c1_1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_image1)\n",
        "    c1_1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1_1)\n",
        "    c1_1 = Dropout(0.3)(c1_1)  # Dropout after convolutional blocks\n",
        "    p1_1 = MaxPooling2D((2, 2))(c1_1)\n",
        "\n",
        "    c2_1 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1_1)\n",
        "    c2_1 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2_1)\n",
        "    c2_1 = Dropout(0.3)(c2_1)  # Dropout after convolutional blocks\n",
        "    p2_1 = MaxPooling2D((2, 2))(c2_1)\n",
        "\n",
        "    c3_1 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2_1)\n",
        "    c3_1 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3_1)\n",
        "    c3_1 = Dropout(0.3)(c3_1)  # Dropout after convolutional blocks\n",
        "    p3_1 = MaxPooling2D((2, 2))(c3_1)\n",
        "\n",
        "    # Encoder for image 2 (same as image 1)\n",
        "    c1_2 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_image2)\n",
        "    c1_2 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1_2)\n",
        "    c1_2 = Dropout(0.3)(c1_2)  # Dropout after convolutional blocks\n",
        "    p1_2 = MaxPooling2D((2, 2))(c1_2)\n",
        "\n",
        "    c2_2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1_2)\n",
        "    c2_2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2_2)\n",
        "    c2_2 = Dropout(0.3)(c2_2)  # Dropout after convolutional blocks\n",
        "    p2_2 = MaxPooling2D((2, 2))(c2_2)\n",
        "\n",
        "    c3_2 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2_2)\n",
        "    c3_2 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3_2)\n",
        "    c3_2 = Dropout(0.3)(c3_2)  # Dropout after convolutional blocks\n",
        "    p3_2 = MaxPooling2D((2, 2))(c3_2)\n",
        "\n",
        "    # Bottleneck (concatenate the features from both images)\n",
        "    c4_1 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3_1)\n",
        "    c4_1 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4_1)\n",
        "    c4_1 = Dropout(0.4)(c4_1)  # Dropout after convolutional blocks\n",
        "\n",
        "    c4_2 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3_2)\n",
        "    c4_2 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4_2)\n",
        "    c4_2 = Dropout(0.4)(c4_2)  # Dropout after convolutional blocks\n",
        "\n",
        "    # Combine the features of both images at the bottleneck\n",
        "    c4 = concatenate([c4_1, c4_2])\n",
        "\n",
        "    # Decoder\n",
        "    u5 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)\n",
        "    u5 = concatenate([u5, c3_1, c3_2])  # Skip connection from both images encoders\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(u5)\n",
        "    c5 = Dropout(0.3)(c5)\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(c5)\n",
        "    c5 = Dropout(0.3)(c5)\n",
        "\n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = concatenate([u6, c2_1, c2_2])\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = Dropout(0.3)(c6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(c6)\n",
        "    c6 = Dropout(0.3)(c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = concatenate([u7, c1_1, c1_2])\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = Dropout(0.3)(c7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(c7)\n",
        "    c7 = Dropout(0.3)(c7)\n",
        "\n",
        "    # Final output\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c7)\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "go-Zdn9r3uFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unet_with_resnet(input_shape=(256, 256, 6)):\n",
        "    # Split the input into two separate images (image1 and image2)\n",
        "    inputs = Input(input_shape)\n",
        "    input_image1 = inputs[..., :3]  # First 3 channels\n",
        "    input_image2 = inputs[..., 3:]  # Last 3 channels\n",
        "\n",
        "    # Pretrained ResNet50 as the backbone\n",
        "    base_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(256, 256, 3))\n",
        "\n",
        "    # Extract encoder layers\n",
        "    encoder_layers1 = [\n",
        "        base_model.get_layer(\"conv1_relu\").output,\n",
        "        base_model.get_layer(\"conv2_block3_out\").output,\n",
        "        base_model.get_layer(\"conv3_block4_out\").output,\n",
        "        base_model.get_layer(\"conv4_block6_out\").output,\n",
        "        base_model.get_layer(\"conv5_block3_out\").output\n",
        "    ]\n",
        "\n",
        "    # Create the encoder model for image1\n",
        "    encoder1 = Model(inputs=base_model.input, outputs=encoder_layers1)\n",
        "    encoder1.trainable = False  # Freeze pretrained layers initially\n",
        "\n",
        "    # Extract features for image1\n",
        "    encoder_outputs1 = encoder1(input_image1)\n",
        "\n",
        "    # Extract features for image2 using the same encoder\n",
        "    encoder_outputs2 = encoder1(input_image2)\n",
        "\n",
        "    # Combine encoder outputs\n",
        "    combined_features = [\n",
        "        Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]), name=f\"abs_diff_{i}\")([feat1, feat2])\n",
        "        for i, (feat1, feat2) in enumerate(zip(encoder_outputs1, encoder_outputs2))\n",
        "    ]\n",
        "\n",
        "    # Decoder with skip connections\n",
        "    c1, c2, c3, c4, c5 = combined_features\n",
        "\n",
        "    # Decoder 1\n",
        "    u6 = UpSampling2D((2, 2))(c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = Dropout(0.3)(c6)  # Add Dropout here\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "    c6 = Dropout(0.3)(c6)  # Add Dropout here\n",
        "\n",
        "    # Decoder 2\n",
        "    u7 = UpSampling2D((2, 2))(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = Dropout(0.3)(c7)  # Add Dropout here\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "    c7 = Dropout(0.3)(c7)  # Add Dropout here\n",
        "\n",
        "    # Decoder 3\n",
        "    u8 = UpSampling2D((2, 2))(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = Dropout(0.3)(c8)  # Add Dropout here\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "    c8 = Dropout(0.3)(c8)  # Add Dropout here\n",
        "\n",
        "    # Decoder 4\n",
        "    u9 = UpSampling2D((2, 2))(c8)\n",
        "    u9 = concatenate([u9, c1])\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = Dropout(0.3)(c9)  # Add Dropout here\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
        "    c9 = Dropout(0.3)(c9)  # Add Dropout here\n",
        "\n",
        "    # Final output\n",
        "    u10 = UpSampling2D((2, 2))(c9)\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(u10)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "BiSR3qBd3uCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# ===============================\n",
        "# Build SiHDNet-like UNet model\n",
        "def build_sihdnet_model(input_size=(256, 256, 6)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Split the input into two separate RGB images\n",
        "    input_image1 = inputs[..., :3]\n",
        "    input_image2 = inputs[..., 3:]\n",
        "\n",
        "    # ---------------- Encoder for image 1 ----------------\n",
        "    def encoder_block(x, filters):\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(x)\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(c)\n",
        "        c = Dropout(0.3)(c)\n",
        "        p = MaxPooling2D((2,2))(c)\n",
        "        return c, p\n",
        "\n",
        "    c1_1, p1_1 = encoder_block(input_image1, 64)\n",
        "    c2_1, p2_1 = encoder_block(p1_1, 128)\n",
        "    c3_1, p3_1 = encoder_block(p2_1, 256)\n",
        "\n",
        "    # ---------------- Encoder for image 2 ----------------\n",
        "    c1_2, p1_2 = encoder_block(input_image2, 64)\n",
        "    c2_2, p2_2 = encoder_block(p1_2, 128)\n",
        "    c3_2, p3_2 = encoder_block(p2_2, 256)\n",
        "\n",
        "    # ---------------- Bottleneck ----------------\n",
        "    c4_1 = Conv2D(512, (3,3), activation='relu', padding='same')(p3_1)\n",
        "    c4_1 = Conv2D(512, (3,3), activation='relu', padding='same')(c4_1)\n",
        "    c4_1 = Dropout(0.4)(c4_1)\n",
        "\n",
        "    c4_2 = Conv2D(512, (3,3), activation='relu', padding='same')(p3_2)\n",
        "    c4_2 = Conv2D(512, (3,3), activation='relu', padding='same')(c4_2)\n",
        "    c4_2 = Dropout(0.4)(c4_2)\n",
        "\n",
        "    # Combine features from both images\n",
        "    c4 = concatenate([c4_1, c4_2])\n",
        "\n",
        "    # ---------------- Decoder ----------------\n",
        "    def decoder_block(x, skip1, skip2, filters):\n",
        "        u = Conv2DTranspose(filters, (2,2), strides=(2,2), padding='same')(x)\n",
        "        u = concatenate([u, skip1, skip2])\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(u)\n",
        "        c = Dropout(0.3)(c)\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(c)\n",
        "        c = Dropout(0.3)(c)\n",
        "        return c\n",
        "\n",
        "    c5 = decoder_block(c4, c3_1, c3_2, 256)\n",
        "    c6 = decoder_block(c5, c2_1, c2_2, 128)\n",
        "    c7 = decoder_block(c6, c1_1, c1_2, 64)\n",
        "\n",
        "    # ---------------- Output ----------------\n",
        "    outputs = Conv2D(1, (1,1), activation='sigmoid')(c7)\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# ===============================\n",
        "# Instantiate SiHDNet model\n",
        "model_sihdnet = build_sihdnet_model(input_size=(256, 256, 6))\n"
      ],
      "metadata": {
        "id": "PhZ4Mgk63uAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# ===============================\n",
        "# Build GLAI-Net-like UNet model\n",
        "def build_glainet_model(input_size=(256, 256, 6)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Split the input into two separate RGB images\n",
        "    input_image1 = inputs[..., :3]\n",
        "    input_image2 = inputs[..., 3:]\n",
        "\n",
        "    # ---------------- Encoder for image 1 ----------------\n",
        "    c1_1 = Conv2D(64, (3,3), activation='relu', padding='same')(input_image1)\n",
        "    c1_1 = Conv2D(64, (3,3), activation='relu', padding='same')(c1_1)\n",
        "    c1_1 = Dropout(0.3)(c1_1)\n",
        "    p1_1 = MaxPooling2D((2,2))(c1_1)\n",
        "\n",
        "    c2_1 = Conv2D(128, (3,3), activation='relu', padding='same')(p1_1)\n",
        "    c2_1 = Conv2D(128, (3,3), activation='relu', padding='same')(c2_1)\n",
        "    c2_1 = Dropout(0.3)(c2_1)\n",
        "    p2_1 = MaxPooling2D((2,2))(c2_1)\n",
        "\n",
        "    c3_1 = Conv2D(256, (3,3), activation='relu', padding='same')(p2_1)\n",
        "    c3_1 = Conv2D(256, (3,3), activation='relu', padding='same')(c3_1)\n",
        "    c3_1 = Dropout(0.3)(c3_1)\n",
        "    p3_1 = MaxPooling2D((2,2))(c3_1)\n",
        "\n",
        "    # ---------------- Encoder for image 2 ----------------\n",
        "    c1_2 = Conv2D(64, (3,3), activation='relu', padding='same')(input_image2)\n",
        "    c1_2 = Conv2D(64, (3,3), activation='relu', padding='same')(c1_2)\n",
        "    c1_2 = Dropout(0.3)(c1_2)\n",
        "    p1_2 = MaxPooling2D((2,2))(c1_2)\n",
        "\n",
        "    c2_2 = Conv2D(128, (3,3), activation='relu', padding='same')(p1_2)\n",
        "    c2_2 = Conv2D(128, (3,3), activation='relu', padding='same')(c2_2)\n",
        "    c2_2 = Dropout(0.3)(c2_2)\n",
        "    p2_2 = MaxPooling2D((2,2))(c2_2)\n",
        "\n",
        "    c3_2 = Conv2D(256, (3,3), activation='relu', padding='same')(p2_2)\n",
        "    c3_2 = Conv2D(256, (3,3), activation='relu', padding='same')(c3_2)\n",
        "    c3_2 = Dropout(0.3)(c3_2)\n",
        "    p3_2 = MaxPooling2D((2,2))(c3_2)\n",
        "\n",
        "    # ---------------- Bottleneck ----------------\n",
        "    c4_1 = Conv2D(512, (3,3), activation='relu', padding='same')(p3_1)\n",
        "    c4_1 = Conv2D(512, (3,3), activation='relu', padding='same')(c4_1)\n",
        "    c4_1 = Dropout(0.4)(c4_1)\n",
        "\n",
        "    c4_2 = Conv2D(512, (3,3), activation='relu', padding='same')(p3_2)\n",
        "    c4_2 = Conv2D(512, (3,3), activation='relu', padding='same')(c4_2)\n",
        "    c4_2 = Dropout(0.4)(c4_2)\n",
        "\n",
        "    c4 = concatenate([c4_1, c4_2])\n",
        "\n",
        "    # ---------------- Decoder ----------------\n",
        "    u5 = Conv2DTranspose(256, (2,2), strides=(2,2), padding='same')(c4)\n",
        "    u5 = concatenate([u5, c3_1, c3_2])\n",
        "    c5 = Conv2D(256, (3,3), activation='relu', padding='same')(u5)\n",
        "    c5 = Dropout(0.3)(c5)\n",
        "    c5 = Conv2D(256, (3,3), activation='relu', padding='same')(c5)\n",
        "    c5 = Dropout(0.3)(c5)\n",
        "\n",
        "    u6 = Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(c5)\n",
        "    u6 = concatenate([u6, c2_1, c2_2])\n",
        "    c6 = Conv2D(128, (3,3), activation='relu', padding='same')(u6)\n",
        "    c6 = Dropout(0.3)(c6)\n",
        "    c6 = Conv2D(128, (3,3), activation='relu', padding='same')(c6)\n",
        "    c6 = Dropout(0.3)(c6)\n",
        "\n",
        "    u7 = Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(c6)\n",
        "    u7 = concatenate([u7, c1_1, c1_2])\n",
        "    c7 = Conv2D(64, (3,3), activation='relu', padding='same')(u7)\n",
        "    c7 = Dropout(0.3)(c7)\n",
        "    c7 = Conv2D(64, (3,3), activation='relu', padding='same')(c7)\n",
        "    c7 = Dropout(0.3)(c7)\n",
        "\n",
        "    # ---------------- Output ----------------\n",
        "    outputs = Conv2D(1, (1,1), activation='sigmoid')(c7)\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "K9-ErUYm3t9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, Multiply, Add\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# ===============================\n",
        "# Build MTFSR-like UNet model with simple Dynamic Attention Unit\n",
        "def build_mtfsr_model(input_size=(256, 256, 6)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Split the input into two separate RGB images\n",
        "    input_image1 = inputs[..., :3]\n",
        "    input_image2 = inputs[..., 3:]\n",
        "\n",
        "    # ---------------- Encoder for image 1 ----------------\n",
        "    def encoder_block(x, filters):\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(x)\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(c)\n",
        "        c = Dropout(0.3)(c)\n",
        "        p = MaxPooling2D((2,2))(c)\n",
        "        return c, p\n",
        "\n",
        "    c1_1, p1_1 = encoder_block(input_image1, 64)\n",
        "    c2_1, p2_1 = encoder_block(p1_1, 128)\n",
        "    c3_1, p3_1 = encoder_block(p2_1, 256)\n",
        "\n",
        "    # ---------------- Encoder for image 2 ----------------\n",
        "    c1_2, p1_2 = encoder_block(input_image2, 64)\n",
        "    c2_2, p2_2 = encoder_block(p1_2, 128)\n",
        "    c3_2, p3_2 = encoder_block(p2_2, 256)\n",
        "\n",
        "    # ---------------- Bottleneck ----------------\n",
        "    c4_1 = Conv2D(512, (3,3), activation='relu', padding='same')(p3_1)\n",
        "    c4_1 = Conv2D(512, (3,3), activation='relu', padding='same')(c4_1)\n",
        "    c4_1 = Dropout(0.4)(c4_1)\n",
        "\n",
        "    c4_2 = Conv2D(512, (3,3), activation='relu', padding='same')(p3_2)\n",
        "    c4_2 = Conv2D(512, (3,3), activation='relu', padding='same')(c4_2)\n",
        "    c4_2 = Dropout(0.4)(c4_2)\n",
        "\n",
        "    # ---------------- Dynamic Attention Unit ----------------\n",
        "    attention = Multiply()([c4_1, c4_2])  # Simple feature interaction\n",
        "    c4 = Add()([c4_1, c4_2, attention])  # Combine with residual\n",
        "\n",
        "    # ---------------- Decoder ----------------\n",
        "    def decoder_block(x, skip1, skip2, filters):\n",
        "        u = Conv2DTranspose(filters, (2,2), strides=(2,2), padding='same')(x)\n",
        "        u = concatenate([u, skip1, skip2])\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(u)\n",
        "        c = Dropout(0.3)(c)\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(c)\n",
        "        c = Dropout(0.3)(c)\n",
        "        return c\n",
        "\n",
        "    c5 = decoder_block(c4, c3_1, c3_2, 256)\n",
        "    c6 = decoder_block(c5, c2_1, c2_2, 128)\n",
        "    c7 = decoder_block(c6, c1_1, c1_2, 64)\n",
        "\n",
        "    # ---------------- Output ----------------\n",
        "    outputs = Conv2D(1, (1,1), activation='sigmoid')(c7)\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "6I4hXboQ3t7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# ===============================\n",
        "# Build SiHDNet-like UNet model\n",
        "def build_sifanet_model(input_size=(256, 256, 6)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Split the input into two separate RGB images\n",
        "    input_image1 = inputs[..., :3]\n",
        "    input_image2 = inputs[..., 3:]\n",
        "\n",
        "    # ---------------- Encoder for image 1 ----------------\n",
        "    def encoder_block(x, filters):\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(x)\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(c)\n",
        "        c = Dropout(0.3)(c)\n",
        "        p = MaxPooling2D((2,2))(c)\n",
        "        return c, p\n",
        "\n",
        "    c1_1, p1_1 = encoder_block(input_image1, 64)\n",
        "    c2_1, p2_1 = encoder_block(p1_1, 128)\n",
        "    c3_1, p3_1 = encoder_block(p2_1, 256)\n",
        "\n",
        "    # ---------------- Encoder for image 2 ----------------\n",
        "    c1_2, p1_2 = encoder_block(input_image2, 64)\n",
        "    c2_2, p2_2 = encoder_block(p1_2, 128)\n",
        "    c3_2, p3_2 = encoder_block(p2_2, 256)\n",
        "\n",
        "    # ---------------- Bottleneck ----------------\n",
        "    c4_1 = Conv2D(512, (3,3), activation='relu', padding='same')(p3_1)\n",
        "    c4_1 = Conv2D(512, (3,3), activation='relu', padding='same')(c4_1)\n",
        "    c4_1 = Dropout(0.4)(c4_1)\n",
        "\n",
        "    c4_2 = Conv2D(512, (3,3), activation='relu', padding='same')(p3_2)\n",
        "    c4_2 = Conv2D(512, (3,3), activation='relu', padding='same')(c4_2)\n",
        "    c4_2 = Dropout(0.4)(c4_2)\n",
        "\n",
        "    # Combine features from both images\n",
        "    c4 = concatenate([c4_1, c4_2])\n",
        "\n",
        "    # ---------------- Decoder ----------------\n",
        "    def decoder_block(x, skip1, skip2, filters):\n",
        "        u = Conv2DTranspose(filters, (2,2), strides=(2,2), padding='same')(x)\n",
        "        u = concatenate([u, skip1, skip2])\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(u)\n",
        "        c = Dropout(0.3)(c)\n",
        "        c = Conv2D(filters, (3,3), activation='relu', padding='same')(c)\n",
        "        c = Dropout(0.3)(c)\n",
        "        return c\n",
        "\n",
        "    c5 = decoder_block(c4, c3_1, c3_2, 256)\n",
        "    c6 = decoder_block(c5, c2_1, c2_2, 128)\n",
        "    c7 = decoder_block(c6, c1_1, c1_2, 64)\n",
        "\n",
        "    # ---------------- Output ----------------\n",
        "    outputs = Conv2D(1, (1,1), activation='sigmoid')(c7)\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "IGzXCLkn3t4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (256, 256, 6)"
      ],
      "metadata": {
        "id": "vlGYyNyz3t2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_unet = build_unet_model(input_shape)\n",
        "model_unet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_unet.summary()"
      ],
      "metadata": {
        "id": "E96dPpne3tzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_unet_with_resnet = build_unet_with_resnet(input_shape)\n",
        "model_unet_with_resnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_unet_with_resnet.summary()"
      ],
      "metadata": {
        "id": "9UDjbeea3twp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Instantiate SiHDNet model\n",
        "model_sihdnet = build_sihdnet_model(input_size=(256, 256, 6))\n",
        "\n",
        "# Compile the model\n",
        "model_sihdnet.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Show model summary\n",
        "model_sihdnet.summary()"
      ],
      "metadata": {
        "id": "k0Q9-DLU3tuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Instantiate GLAI-Net model\n",
        "model_glainet = build_glainet_model(input_size=(256, 256, 6))\n",
        "\n",
        "# Compile the model\n",
        "model_glainet.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Show model summary\n",
        "model_glainet.summary()"
      ],
      "metadata": {
        "id": "3s3MXmdr3tr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Instantiate MTFSR model\n",
        "model_mtfsr = build_mtfsr_model(input_size=(256, 256, 6))\n",
        "\n",
        "# Compile the model\n",
        "model_mtfsr.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Show model summary\n",
        "model_mtfsr.summary()"
      ],
      "metadata": {
        "id": "_oCO-7iZ3tpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Instantiate SifaNet model\n",
        "model_sifanet = build_sifanet_model(input_size=(256, 256, 6))\n",
        "\n",
        "# Compile the model\n",
        "model_sifanet.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Show model summary\n",
        "model_sifanet.summary()"
      ],
      "metadata": {
        "id": "oPAFSqNb3tm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5mT8Y2xl3tka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Paths to LEVIR-CD+ dataset\n",
        "image1_train_dir = \"/kaggle/input/levir-cd-change-detection/LEVIR-CD+/train/A\"\n",
        "image2_train_dir =  \"/kaggle/input/levir-cd-change-detection/LEVIR-CD+/train/B\"\n",
        "mask_train_dir = \"/kaggle/input/levir-cd-change-detection/LEVIR-CD+/train/label\"\n",
        "\n",
        "image1_test_dir =  \"/kaggle/input/levir-cd-change-detection/LEVIR-CD+/train/A\"\n",
        "image2_test_dir =  \"/kaggle/input/levir-cd-change-detection/LEVIR-CD+/train/B\"\n",
        "mask_test_dir = \"/kaggle/input/levir-cd-change-detection/LEVIR-CD+/train/label\"\n",
        "\n",
        "input_shape = (256, 256)  # Resize dimensions\n",
        "\n",
        "def load_images(image1_dir, image2_dir, mask_dir):\n",
        "    image1_files = sorted(os.listdir(image1_dir))\n",
        "    image2_files = sorted(os.listdir(image2_dir))\n",
        "    mask_files = sorted(os.listdir(mask_dir))\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for img1, img2, mask in zip(image1_files, image2_files, mask_files):\n",
        "\n",
        "        img1_path = os.path.join(image1_dir, img1)\n",
        "        img2_path = os.path.join(image2_dir, img2)\n",
        "        mask_path = os.path.join(mask_dir, mask)\n",
        "\n",
        "        img1 = cv2.imread(img1_path)\n",
        "        img2 = cv2.imread(img2_path)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Resize images and mask\n",
        "        img1 = cv2.resize(img1, input_shape)\n",
        "        img2 = cv2.resize(img2, input_shape)\n",
        "        mask = cv2.resize(mask, input_shape)\n",
        "\n",
        "        # Normalize images and mask\n",
        "        img1 = img1 / 255.0\n",
        "        img2 = img2 / 255.0\n",
        "        mask = mask / 255.0\n",
        "\n",
        "        # Stack images along the channel axis\n",
        "        stacked_image = np.concatenate([img1, img2], axis=-1)  # Shape: (256, 256, 6)\n",
        "\n",
        "        X.append(stacked_image)\n",
        "        y.append(mask)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Load the dataset\n",
        "X, y = load_images(image1_train_dir, image2_train_dir, mask_train_dir)\n",
        "X_test, y_test = load_images(image1_test_dir, image2_test_dir, mask_test_dir)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Validation set size:\", X_val.shape)\n",
        "print(\"Test set size:\", X_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "o78O8IVX3th4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "callbacks_unet = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(\"best_change_detection_unet_model.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "mVTJK9oA3tff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks_unet_with_resnet = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(\"best_change_detection_unet_resnet_model.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "]\n"
      ],
      "metadata": {
        "id": "kfJg5Ggh3tc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Callbacks for training\n",
        "callbacks_sihdnet = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(\"best_change_detection_sihdnet_model.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "]"
      ],
      "metadata": {
        "id": "JEF6pGf33tag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===============================\n",
        "# Callbacks for training\n",
        "callbacks_glainet = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(\"best_change_detection_glainet_model.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "]\n"
      ],
      "metadata": {
        "id": "wgNrF4Lp3tX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Callbacks for training\n",
        "callbacks_mtfsr = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(\"best_change_detection_mtfsr_model.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "]"
      ],
      "metadata": {
        "id": "9TBJqyiw3tVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Callbacks for training\n",
        "callbacks_sifanet = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(\"best_change_detection_sihdnet_model.keras\", monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "]"
      ],
      "metadata": {
        "id": "2Wr7My6j3tS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_unet = model_unet.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=8,\n",
        "    epochs=25,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks_unet\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Lpi7DEDu3tQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history_unet_with_resnet = model_unet_with_resnet.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=8,\n",
        "    epochs=25,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks_unet_with_resnet\n",
        ")\n"
      ],
      "metadata": {
        "id": "i811Jq1y3tN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Training the model\n",
        "history_sihdnet = model_sihdnet.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=8,\n",
        "    epochs=25,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks_sihdnet\n",
        ")\n"
      ],
      "metadata": {
        "id": "pP_HSsB83tLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Training the model\n",
        "history_glainet = model_glainet.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=8,\n",
        "    epochs=25,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks_glainet\n",
        ")"
      ],
      "metadata": {
        "id": "34QJAVoF3tJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Training the model\n",
        "history_mtfsr = model_mtfsr.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=8,\n",
        "    epochs=25,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks_mtfsr\n",
        ")"
      ],
      "metadata": {
        "id": "k6hyNS4F3tGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Training the model\n",
        "history_sifanet = model_sifanet.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=8,\n",
        "    epochs=25,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks_sifanet\n",
        ")\n"
      ],
      "metadata": {
        "id": "687pVX2p3tD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['U-Net model', 'U-Net with RES-Net model','sihdnet model','glainet model','mtfsr model','sihdnet model']\n",
        "history = [history_unet, history_unet_with_resnet ,history_sihdnet, history_glainet, history_mtfsr,history_sihdnet]\n",
        "for i in range(6):\n",
        "    for metric in ['loss', 'accuracy']:\n",
        "        plt.plot(history[i].history[metric], label='Training Loss')\n",
        "        plt.plot(history[i].history['val_' + metric], label='Validation Loss')\n",
        "        plt.title(models[i] + ' ' + metric)\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel(metric)\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Gd8RXYrx3tBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model_unet_with_resnet.evaluate(X_test, y_test, batch_size=8, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predict on test set\n",
        "predictions_unet_with_resnet = model_unet_with_resnet.predict(X_test, batch_size=8, verbose=1)\n",
        "#predicted_masks = (predictions > 0.5).astype(int)\n",
        "\n"
      ],
      "metadata": {
        "id": "QHA2lCii3s-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model_unet.evaluate(X_test, y_test, batch_size=8, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predict on test set\n",
        "predictions_unet = model_unet.predict(X_test, batch_size=8, verbose=1)\n",
        "#predicted_masks = (predictions > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "ojIySMka3s8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model_sihdnet.evaluate(X_test, y_test, batch_size=8, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predict on test set\n",
        "predictions_unet_sihdnet = model_sihdnet.predict(X_test, batch_size=8, verbose=1)\n",
        "#predicted_masks = (predictions > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "Po4dTEQh3s5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model_glainet.evaluate(X_test, y_test, batch_size=8, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predict on test set\n",
        "predictions_unet_glainet = model_glainet.predict(X_test, batch_size=8, verbose=1)\n",
        "#predicted_masks = (predictions > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "NI3RG4bO3s3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model_mtfsr.evaluate(X_test, y_test, batch_size=8, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predict on test set\n",
        "predictions_unet_mtfsr = model_mtfsr.predict(X_test, batch_size=8, verbose=1)\n",
        "#predicted_masks = (predictions > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "WmeWcVNo3s0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model_sifanet.evaluate(X_test, y_test, batch_size=8, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predict on test set\n",
        "predictions_unet_sifanet = model_sifanet.predict(X_test, batch_size=8, verbose=1)\n",
        "#predicted_masks = (predictions > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "7E3QFYDF3syb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def overlay_mask(image, mask, color=(255, 0, 0), alpha=0.5, max_val = 0.2):\n",
        "\n",
        "    mask_colored = np.zeros_like(image, dtype=np.uint8)\n",
        "    mask_colored[mask > max_val] = color\n",
        "\n",
        "    overlayed = cv2.addWeighted(image, 1, mask_colored, alpha, 0)\n",
        "\n",
        "    return overlayed\n",
        "\n",
        "\n",
        "predictions = [predictions_unet_with_resnet, predictions_unet,predictions_unet_sihdnet,predictions_unet_glainet,predictions_unet_mtfsr,predictions_unet_sifanet]\n",
        "models = ['U-Net model', 'U-Net with RES-Net model','sihdnet model','glainet model','mtfsr model','sihdnet model']\n",
        "\n",
        "for k in range(6):\n",
        "    print('-----------------------------'+ models[k] +'----------------------------------------')\n",
        "\n",
        "    # Visualize results\n",
        "    for i in range(6):\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 4, 1)\n",
        "        plt.imshow(X_test[i, :, :, :3])\n",
        "        plt.title(\"Input Image 1\")\n",
        "        plt.subplot(1, 4, 2)\n",
        "        plt.imshow(X_test[i, :, :, 3:])\n",
        "        plt.title(\"Input Image 2\")\n",
        "        plt.subplot(1, 4, 3)\n",
        "        plt.imshow(y_test[i].squeeze(), cmap='gray')\n",
        "        plt.title(\"Ground Truth Mask\")\n",
        "        plt.subplot(1, 4, 4)\n",
        "        plt.imshow(predictions[k][i].squeeze(), cmap='gray')\n",
        "        plt.title(\"Predicted Mask\")\n",
        "        plt.show()\n",
        "        print(f\"Predictions Min: {predictions[k][i].min()}, Max: {predictions[k][i].max()}\")\n",
        "\n",
        "\n",
        "        # Overlay mask on image\n",
        "        overlayed_image_1 = overlay_mask((X_test[i, :, :, :3] * 255).astype(np.uint8), predictions[k][i].squeeze(), color=(255, 0, 0), alpha=0.5)\n",
        "        overlayed_image_2 = overlay_mask((X_test[i, :, :, 3:] * 255).astype(np.uint8), predictions[k][i].squeeze(), color=(255, 0, 0), alpha=0.5)\n",
        "\n",
        "        # Display results\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(overlayed_image_1)\n",
        "        plt.title(\"Overlayed Image - 1\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(overlayed_image_2)\n",
        "        plt.title(\"Overlayed Image - 2\")\n",
        "\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "3y5gtrV83swJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to save the best model\n",
        "best_model_path = '/kaggle/working/sihdnet_best_model.h5'\n",
        "\n",
        "# Check if model already exists\n",
        "if os.path.exists(best_model_path):\n",
        "    print(\"Best model is already saved\")\n",
        "else:\n",
        "    model_sihdnet.save(best_model_path)\n",
        "    print(\"Best model (SiHDNet) saved successfully\")"
      ],
      "metadata": {
        "id": "pYf_fDNj3stv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_weights_path = '/kaggle/working/sihdnet_best_model_weights.weights.h5'\n",
        "import os\n",
        "\n",
        "# Path to save the best model weights\n",
        "best_weights_path = '/kaggle/working/sihdnet_best_model_weights.weights.h5'\n",
        "\n",
        "# Check if weights file already exists\n",
        "if os.path.exists(best_weights_path):\n",
        "    print(\"Best model weights are already saved\")\n",
        "else:\n",
        "    # Save only weights of the best model (SiHDNet)\n",
        "    model_sihdnet.save_weights(best_weights_path)\n",
        "    print(\"Best model weights (SiHDNet) saved successfully\")"
      ],
      "metadata": {
        "id": "EOVC20jk3srd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}